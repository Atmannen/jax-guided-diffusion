{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Huemin Jax Diffusion 2.7 ",
      "provenance": [],
      "collapsed_sections": [
        "QJ5aW34zTJBX"
      ],
      "private_outputs": true,
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D-DD0F0u_SY"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path"
      ],
      "metadata": {
        "id": "YyRRmMGdSdZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Default Path Variables:\n",
        "models_path = \"/content/models/\" #@param {type:\"string\"}\n",
        "output_path = \"/content/output/\" #@param {type:\"string\"}\n",
        "link_path = \"/content/link/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Mount Google Drive (Optional):\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "force_remount = False #@param {type:\"boolean\"}\n",
        "\n",
        "if mount_google_drive:\n",
        "  from google.colab import drive\n",
        "  try:\n",
        "    drive_path = \"/content/drive\" #@param {type:\"string\"}\n",
        "    drive.mount(drive_path,force_remount=force_remount)\n",
        "    models_path = \"/content/drive/MyDrive/AI/models/\" #@param {type:\"string\"}\n",
        "    output_path = \"/content/drive/MyDrive/AI/JAX/\" #@param {type:\"string\"}\n",
        "    link_path = \"/content/drive/MyDrive/AI/JAX/link/\" #@param {type:\"string\"}\n",
        "  except:\n",
        "    print(\"...error mounting drive or with drive path variables\")\n",
        "    print(\"...reverting to default path variables\")\n",
        "    models_path = \"/content/models/\"\n",
        "    output_path = \"/content/output/\"\n",
        "    link_path = \"/content/link/\"    \n",
        "\n",
        "!mkdir -p $models_path\n",
        "!mkdir -p $output_path\n",
        "!mkdir -p $link_path\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")\n",
        "print(f\"link_path: {link_path}\")"
      ],
      "metadata": {
        "id": "DcaT389kvakt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup + Definitions"
      ],
      "metadata": {
        "id": "QJ5aW34zTJBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download repositories and patches"
      ],
      "metadata": {
        "id": "qoI_KUhtTTer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.system(\"nvidia-smi | grep A100\") == 0:\n",
        "  !pip install -U https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.72+cuda111-cp37-none-manylinux2010_x86_64.whl \"jax==0.2.25\"\n",
        "else:\n",
        "  !pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.75%2Bcuda11.cudnn805-cp37-none-manylinux2010_x86_64.whl \"jax==0.3.0\"\n",
        "!pip install dm-haiku==0.0.5 cbor2 ftfy einops braceexpand\n",
        "!git clone https://github.com/huemin-art/CLIP_JAX\n",
        "!git clone https://github.com/huemin-art/jax-guided-diffusion -b v2.7\n",
        "!git clone https://github.com/huemin-art/v-diffusion-jax"
      ],
      "metadata": {
        "id": "24bGVHTgTDrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import python libraries"
      ],
      "metadata": {
        "id": "Cw_jQBhoTiMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "sys.path.append('./v-diffusion-jax')\n",
        "\n",
        "import io\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "from subprocess import Popen\n",
        "from functools import partial\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxtorch import PRNG\n",
        "\n",
        "import torch\n",
        "from torch import tensor\n",
        "from torchvision import utils\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "from diffusion_models.schedules import cosine, spliced\n",
        "from diffusion_models.common import norm1, blur_fft, make_partial\n",
        "from diffusion_models.lazy import LazyParams\n",
        "from diffusion_models.perceptor import get_clip, normalize\n",
        "from diffusion_models.cc12m_1 import cc12m_1_wrap\n",
        "from diffusion_models.openai import openai_512_finetune\n",
        "from diffusion_models.secondary import secondary2_wrap\n",
        "from diffusion_models import sampler"
      ],
      "metadata": {
        "id": "l213ok1iS4H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fetch functions"
      ],
      "metadata": {
        "id": "XiKk9OvRTpA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    local_path = os.path.join(models_path, basename)\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    else:\n",
        "        os.makedirs(f'{models_path}tmp', exist_ok=True)\n",
        "        Popen(['curl', '--http1.1', url_or_path, '-o', f'{models_path}tmp/{basename}']).wait()\n",
        "        os.rename(f'{models_path}tmp/{basename}', local_path)\n",
        "        return local_path"
      ],
      "metadata": {
        "id": "B35XNFIZWXrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CC12M model parameters"
      ],
      "metadata": {
        "id": "zVFwGKQ3T3x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cc12m_1_params = LazyParams.pt('https://the-eye.eu/public/AI/models/v-diffusion/cc12m_1.pth')\n",
        "LazyParams.fetch = fetch_model"
      ],
      "metadata": {
        "id": "8Cz5g2FoT2bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP cutout functions"
      ],
      "metadata": {
        "id": "F4I_dOctT-f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grey(image):\n",
        "    [*_, c, h, w] = image.shape\n",
        "    return jnp.broadcast_to(image.mean(axis=-3, keepdims=True), image.shape)\n",
        "\n",
        "def cutout_image(image, offsetx, offsety, size, output_size=224):\n",
        "    \"\"\"Computes (square) cutouts of an image given x and y offsets and size.\"\"\"\n",
        "    (c, h, w) = image.shape\n",
        "\n",
        "    scale = jnp.stack([output_size / size, output_size / size])\n",
        "    translation = jnp.stack([-offsety * output_size / size, -offsetx * output_size / size])\n",
        "    return jax.image.scale_and_translate(image,\n",
        "                                         shape=(c, output_size, output_size),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def cutouts_images(image, offsetx, offsety, size, output_size=224):\n",
        "    f = partial(cutout_image, output_size=output_size)         # [c h w] [] [] [] -> [c h w]\n",
        "    f = jax.vmap(f, in_axes=(0, 0, 0, 0), out_axes=0)          # [n c h w] [n] [n] [n] -> [n c h w]\n",
        "    f = jax.vmap(f, in_axes=(None, 0, 0, 0), out_axes=0)       # [n c h w] [k n] [k n] [k n] -> [k n c h w]\n",
        "    return f(image, offsetx, offsety, size)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.0, p_grey=0.2, p_mixgrey=None, p_flip=0.5):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "        self.p_flip = p_flip\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "\n",
        "        small_cuts = self.cutn//2\n",
        "        large_cuts = self.cutn - self.cutn//2\n",
        "\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[small_cuts, n])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size)).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [small_cuts, n], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "\n",
        "        B1 = 40\n",
        "        B2 = 40\n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[large_cuts, n])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [large_cuts, n], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [large_cuts, n], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        if self.p_mixgrey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        if self.p_grey is not None:\n",
        "          grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "\n",
        "        if self.p_flip is not None:\n",
        "          flip_us = jax.random.bernoulli(rng.split(), self.p_flip, [self.cutn, n, 1, 1, 1])\n",
        "          cutouts = jnp.where(flip_us, jnp.flip(cutouts, axis=-1), cutouts)\n",
        "\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.cut_pow, self.p_grey, self.p_mixgrey, self.p_flip], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        return MakeCutouts(cut_size, cutn, *dynamic)\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n",
        "\n",
        "@make_partial\n",
        "def SphericalDistLoss(text_embed, clip_guidance_scale, image_embeds):\n",
        "    losses = spherical_dist_loss(image_embeds, text_embed).mean(0)\n",
        "    return (clip_guidance_scale * losses).sum()"
      ],
      "metadata": {
        "id": "k6VSEAlLZBgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "process prompt function"
      ],
      "metadata": {
        "id": "qkscEaSeUIN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_prompt(clip,all_prompt):\n",
        "  embeds = []\n",
        "  expands = all_prompt.split(\"|\")\n",
        "  for prompt in expands:\n",
        "    prompt = prompt.strip()\n",
        "    # check url\n",
        "    if \"https:\" in prompt:\n",
        "      tmp = prompt.split(\":\")\n",
        "      # check weight\n",
        "      if len(tmp) == 2:\n",
        "        temp_weight = 1\n",
        "        temp_prompt = prompt\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here1\")\n",
        "        #print(tmp_embed.shape)\n",
        "      if len(tmp) == 3:\n",
        "        temp_prompt = \":\".join(tmp[0:2]).strip()\n",
        "        temp_weight = float(tmp[2].strip())\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here2\")\n",
        "        #print(tmp_embed.shape)\n",
        "    # if not url\n",
        "    else:\n",
        "      # check weight\n",
        "      if ':' in prompt:\n",
        "        tmp = prompt.split(\":\")\n",
        "        temp_prompt = tmp[0].strip()\n",
        "        temp_weight = float(tmp[1].strip())\n",
        "      else:\n",
        "        temp_prompt = prompt\n",
        "        temp_weight = 1\n",
        "      # try path\n",
        "      try:\n",
        "        init_pil = Image.open(fetch(temp_prompt))\n",
        "        tmp_embed = temp_weight * clip.embed_image(init_pil)\n",
        "        if len(tmp_embed.shape) != 1:\n",
        "          tmp_embed = tmp_embed[-1]\n",
        "        embeds.append(tmp_embed)\n",
        "      except:\n",
        "        tmp_embed = temp_weight * clip.embed_text(temp_prompt.strip())\n",
        "        embeds.append(tmp_embed)\n",
        "        #print(\"here4\")\n",
        "        #print(tmp_embed.shape)\n",
        "  return norm1(sum(embeds))\n",
        "\n",
        "def process_prompts(clip, prompts):\n",
        "  return jnp.stack([process_prompt(clip, prompt) for prompt in prompts])\n",
        "\n",
        "def expand(xs, batch_size):\n",
        "  \"\"\"Extend or truncate the list of prompts to the batch size.\"\"\"\n",
        "  return (xs * batch_size)[:batch_size]"
      ],
      "metadata": {
        "id": "bHJNSGTtV1Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "conditional functions"
      ],
      "metadata": {
        "id": "wCY3_TLWUOAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.tree_util.register_pytree_node_class\n",
        "class MainCondFn(object):\n",
        "    # Used to construct the main cond_fn. Accepts a diffusion model which will\n",
        "    # be used for denoising, plus a list of 'conditions' which will\n",
        "    # generate gradient of a loss wrt the denoised, to be summed together.\n",
        "    def __init__(self, diffusion, conditions, blur_amount=None, use='pred'):\n",
        "        self.diffusion = diffusion\n",
        "        self.conditions = [c for c in conditions if c is not None]\n",
        "        self.blur_amount = blur_amount\n",
        "        self.use = use\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, x, cosine_t, key):\n",
        "        if not self.conditions:\n",
        "          return jnp.zeros_like(x)\n",
        "\n",
        "        rng = PRNG(key)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        alphas, sigmas = cosine.to_alpha_sigma(cosine_t)\n",
        "\n",
        "        def denoise(key, x):\n",
        "            pred = self.diffusion(x, cosine_t, key).pred\n",
        "            if self.use == 'pred':\n",
        "                return pred\n",
        "            elif self.use == 'x_in':\n",
        "                return pred * sigmas + x * alphas\n",
        "        (x_in, backward) = jax.vjp(partial(denoise, rng.split()), x)\n",
        "\n",
        "        total = jnp.zeros_like(x_in)\n",
        "        for cond in self.conditions:\n",
        "            total += cond(x_in, rng.split())\n",
        "        if self.blur_amount is not None:\n",
        "          blur_radius = (self.blur_amount * sigmas / alphas).clamp(0.05,512)\n",
        "          total = blur_fft(total, blur_radius.mean())\n",
        "        final_grad = -backward(total)[0]\n",
        "\n",
        "        # clamp gradients to a max of 0.2\n",
        "        magnitude = final_grad.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "        final_grad = final_grad * jnp.where(magnitude > 0.2, 0.2 / magnitude, 1.0)\n",
        "        return final_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.diffusion, self.conditions, self.blur_amount], [self.use]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MainCondFn(*dynamic, *static)\n",
        "\n",
        "def filternone(xs):\n",
        "  return [x for x in xs if x is not None]\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondCLIP(object):\n",
        "    \"\"\"Backward a loss function through clip.\"\"\"\n",
        "    def __init__(self, perceptor, make_cutouts, cut_batches, *losses):\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.cut_batches = cut_batches\n",
        "        self.losses = filternone(losses)\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).rearrange('(k n) c -> k n c', n=n)\n",
        "            return sum(loss_fn(image_embeds) for loss_fn in self.losses)\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.perceptor, self.make_cutouts, self.losses], [self.cut_batches]\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        [perceptor, make_cutouts, losses] = dynamic\n",
        "        [cut_batches] = static\n",
        "        return cls(perceptor, make_cutouts, cut_batches, *losses)\n",
        "\n",
        "@make_partial\n",
        "def CondTV(tv_scale, x_in, key):\n",
        "    def downscale2d(image, f):\n",
        "        [c, n, h, w] = image.shape\n",
        "        return jax.image.resize(image, [c, n, h//f, w//f], method='cubic')\n",
        "\n",
        "    def tv_loss(input):\n",
        "        \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "        x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "        y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "        return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n",
        "\n",
        "    def sum_tv_loss(x_in, f=None):\n",
        "        if f is not None:\n",
        "            x_in = downscale2d(x_in, f)\n",
        "        return tv_loss(x_in).sum() * tv_scale\n",
        "    tv_grad_512 = jax.grad(sum_tv_loss)(x_in)\n",
        "    tv_grad_256 = jax.grad(partial(sum_tv_loss,f=2))(x_in)\n",
        "    tv_grad_128 = jax.grad(partial(sum_tv_loss,f=4))(x_in)\n",
        "    return tv_grad_512 + tv_grad_256 + tv_grad_128\n",
        "\n",
        "@make_partial\n",
        "def CondRange(range_scale, x_in, key):\n",
        "    def range_loss(x_in):\n",
        "        return jnp.abs(x_in - x_in.clamp(minval=-1,maxval=1)).mean()\n",
        "    return range_scale * jax.grad(range_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMean(mean_scale, x_in, key):\n",
        "    def mean_loss(x_in):\n",
        "        return jnp.abs(x_in).mean()\n",
        "    return mean_scale * jax.grad(mean_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondVar(var_scale, x_in, key):\n",
        "    def var_loss(x_in):\n",
        "        return x_in.var()\n",
        "    return var_scale * jax.grad(var_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondHorizontalSymmetry(horizontal_symmetry_scale, x_in, key):\n",
        "    def horizontal_symmetry_loss(x_in):\n",
        "        [n, c, h, w] = x_in.shape\n",
        "        return jnp.abs(x_in[:, :, :, :w//2]-jnp.flip(x_in[:, :, :, w//2:],-1)).mean()\n",
        "    return horizontal_symmetry_scale * jax.grad(horizontal_symmetry_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondVerticalSymmetry(vertical_symmetry_scale, x_in, key):\n",
        "    def vertical_symmetry_loss(x_in):\n",
        "        [n, c, h, w] = x_in.shape\n",
        "        return jnp.abs(x_in[:, :, :h//2, :]-jnp.flip(x_in[:, :, h//2:, :],-2)).mean()\n",
        "    return vertical_symmetry_scale * jax.grad(vertical_symmetry_loss)(x_in)\n",
        "\n",
        "@make_partial\n",
        "def CondMSE(target, mse_scale, x_in, key):\n",
        "    def mse_loss(x_in):\n",
        "        return (x_in - target).square().mean()\n",
        "    return mse_scale * jax.grad(mse_loss)(x_in)"
      ],
      "metadata": {
        "id": "AjtApGjuWtvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QoL functions"
      ],
      "metadata": {
        "id": "_WqbJ2ghUTX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(images):\n",
        "  images = images.add(1).div(2).clamp(0, 1)\n",
        "  images = torch.tensor(np.array(images))\n",
        "  grid = utils.make_grid(images, 4).cpu()\n",
        "  display.display(TF.to_pil_image(grid))\n",
        "  return\n",
        "\n",
        "def load_image(url,image_size,resize=True):\n",
        "    init_array = Image.open(fetch(url)).convert('RGB')\n",
        "    if resize:\n",
        "      init_array = init_array.resize(image_size, Image.LANCZOS)\n",
        "    init_array = jnp.array(TF.to_tensor(init_array)).unsqueeze(0).mul(2).sub(1)\n",
        "    return init_array\n",
        "\n",
        "def get_output_folder(output_path,batch_folder=None):\n",
        "  yearMonth = time.strftime('%Y-%m/')\n",
        "  out_path = output_path+yearMonth\n",
        "  if batch_folder != \"\":\n",
        "    out_path += batch_folder\n",
        "    if out_path[-1] != \"/\":\n",
        "      out_path += \"/\"\n",
        "  os.makedirs(out_path, exist_ok=True)\n",
        "  return out_path"
      ],
      "metadata": {
        "id": "pqeaRRlVeQ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AR functions"
      ],
      "metadata": {
        "id": "X-XBb_zow_MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.tree_util.register_pytree_node_class\n",
        "class asMakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "\n",
        "        cut_us = jnp.ones((self.cutn,1))\n",
        "        sizes = (cut_us * self.cut_size).clamp(self.cut_size)\n",
        "        offsets_x = jax.random.randint(rng.split(), [self.cutn, n], minval=-self.cut_size//2, maxval=w-self.cut_size//2)\n",
        "        offsets_y = jax.random.randint(rng.split(), [self.cutn, n], minval=-self.cut_size//2, maxval=h-self.cut_size//2)\n",
        "        print(offsets_x)\n",
        "        print(offsets_y)\n",
        "        print(sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes, output_size=self.cut_size)\n",
        "\n",
        "        return cutouts, offsets_x, offsets_y\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ((self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        return asMakeCutouts(cut_size, cutn, *dynamic)\n",
        "\n",
        "def as_cutout_image(ar_image, offsetx, offsety, image_size):\n",
        "    c, h, w = ar_image[0].shape\n",
        "    scale = jnp.stack([1,1])\n",
        "    translation = jnp.stack([-offsety, -offsetx])\n",
        "    return jax.image.scale_and_translate(ar_image[0],\n",
        "                                         shape=(c, image_size[1], image_size[0]),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def create_as_alpha(image_size,border,blur_std):\n",
        "  if border > min(image_size):\n",
        "    print(\"border greater than image dimensions\")\n",
        "  if border < 0:\n",
        "    print(\"border should not be negative\")\n",
        "  ar_alpha = jnp.ones((1,3,image_size[1],image_size[0]))\n",
        "  ar_alpha = ar_alpha.at[:,:,:border,:].set(-1*ar_alpha[:,:,:border,:])\n",
        "  ar_alpha = ar_alpha.at[:,:,image_size[1]-border:,:].set(-1*ar_alpha[:,:,image_size[1]-border:,:])\n",
        "  ar_alpha = ar_alpha.at[:,:,border:image_size[1]-border,:border].set(-1*ar_alpha[:,:,border:image_size[1]-border,:border])\n",
        "  ar_alpha = ar_alpha.at[:,:,border:image_size[1]-border,image_size[0]-border:].set(-1*ar_alpha[:,:,border:image_size[1]-border,image_size[0]-border:])\n",
        "  ar_alpha = blur_fft(ar_alpha,blur_std)\n",
        "  return ar_alpha\n",
        "\n",
        "def create_as_offsets(image_size,ar_image_size,overlap_border):\n",
        "  w, h, ar_w, ar_h = image_size[0], image_size[1], ar_image_size[0], ar_image_size[1]\n",
        "  tmp_w = ar_w+overlap_border\n",
        "  tmp_h = ar_h+overlap_border\n",
        "  tmp_x = -overlap_border\n",
        "  tmp_y = -overlap_border\n",
        "  x_offsets = [tmp_x]\n",
        "  y_offsets = [tmp_y]\n",
        "  while (tmp_x+image_size[0]-overlap_border) < ar_image_size[0]:\n",
        "    tmp_x += image_size[0]-overlap_border\n",
        "    x_offsets.append(tmp_x)\n",
        "  while (tmp_y+image_size[1]-overlap_border) < ar_image_size[1]:\n",
        "    tmp_y += image_size[1]-overlap_border\n",
        "    y_offsets.append(tmp_y)\n",
        "  return x_offsets, y_offsets"
      ],
      "metadata": {
        "id": "7zxha1Y_f2QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings (Required)"
      ],
      "metadata": {
        "id": "JxtyHvbya7Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def params():\n",
        "\n",
        "  #@markdown Batch Settings (Required)\n",
        "  seed = 0 #@param\n",
        "  n_batches = 1 #@param {type:\"integer\"}\n",
        "  batch_size = 1 #@param {type:\"integer\"}\n",
        "  batch_folder = \"test\" #@param {type:\"string\"}\n",
        "  save_settings = True #@param {type:\"boolean\"}\n",
        "  #@markdown Diffusion Model Settings (Required)\n",
        "  steps = 100 #@param {type:\"integer\"}\n",
        "  image_size = (512,512) #@param\n",
        "  diffusion_model = \"CC12M\" #@param [\"OpenAIFinetune\", \"CC12M\"]\n",
        "  use_secondary_model = False #@param {type:\"boolean\"}\n",
        "  sample_mode = 'ddim' #@param [\"ddim\", \"plms\", \"prk\"]\n",
        "  ddim_eta = 0.8 #@param\n",
        "  starting_noise = 1.0 #@param\n",
        "  ending_noise = 0.0 #@param\n",
        "  #@markdown CLIP Model Settings (Required)\n",
        "  use_vitb32 = True #@param {type:\"boolean\"}\n",
        "  use_vitb16 = True #@param {type:\"boolean\"}\n",
        "  use_vitl14 = True #@param {type:\"boolean\"}\n",
        "  all_prompts = \"Chasm City gates at night, trending on artstation by Ya | Japanese woodblock print:0.1 | matte colors:0.5 | red:-1\" #@param {type:\"string\"}\n",
        "  all_clip_guidance_scale = 10000 #@param {type:\"integer\"}\n",
        "  #@markdown Multiple Prompt Settings (Optional)\n",
        "  use_multiple_prompts = False #@param {type:\"boolean\"}\n",
        "  vitb16_all_prompt = \"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\" #@param {type:\"string\"}\n",
        "  vitb16_clip_guidance_scale = 10000 #@param {type:\"integer\"}\n",
        "  vitb32_all_prompt = \"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\" #@param {type:\"string\"}\n",
        "  vitb32_clip_guidance_scale = 10000 #@param {type:\"integer\"}\n",
        "  vitl14_all_prompt = \"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\" #@param {type:\"string\"}\n",
        "  vitl14_clip_guidance_scale = 10000 #@param {type:\"integer\"}\n",
        "  #@markdown Cut Settings (Required)\n",
        "  cutn = 4 #@param {type:\"integer\"}\n",
        "  cut_batches =  4#@param {type:\"integer\"}\n",
        "  cut_pow = 1.0 #@param\n",
        "  cut_p_grey = 0.2 #@param\n",
        "  cut_p_flip = 0.5 #@param\n",
        "  cut_p_mixgrey = None #@param\n",
        "  #@markdown Conditonal Settings (Required)\n",
        "  tv_scale = 200 #@param {type:\"integer\"}\n",
        "  range_scale = 100 #@param {type:\"integer\"}\n",
        "  mean_scale = 500 #@param {type:\"integer\"}\n",
        "  var_scale =   -100#@param {type:\"integer\"}\n",
        "  horizontal_symmetry_scale = 0 #@param {type:\"integer\"}\n",
        "  vertical_symmetry_scale = 0 #@param {type:\"integer\"}\n",
        "  #@markdown Initial Image Settings (Optional)\n",
        "  use_init_img = False #@param {type:\"boolean\"}\n",
        "  init_path = \"\" #@param {type:\"string\"}\n",
        "  init_append_path = link_path #@param\n",
        "  init_mse_scale = 0 #@param {type:\"integer\"}\n",
        "  #@markdown Automated Stitching Settings (Optional)\n",
        "  use_automated_stitching = True #@param {type:\"boolean\"}\n",
        "  as_image_size = (1024,1024) #@param\n",
        "  as_alpha_border = 40 #@param\n",
        "  as_alpha_feather = 10 #@param\n",
        "  as_n_pass =  3#@param\n",
        "  as_stitch_overlap = 128 #@param\n",
        "  as_stitch_shift = [0,64] #@param\n",
        "  #@markdown Automated Stitching Initial Image Settings (Optional)\n",
        "  as_init_path = \"001.png\" #@param {type:\"string\"}\n",
        "  as_init_append_path = link_path #@param\n",
        "  return locals()\n",
        "\n",
        "params = params()\n",
        "globals().update(params)"
      ],
      "metadata": {
        "id": "wkkMfdl7-60L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Test Automated Stitching\n",
        "tmp_path = \"/content/tmp/\"\n",
        "!mkdir -p $tmp_path\n",
        "\n",
        "as_img = -1*jnp.ones((1,3,as_image_size[1],as_image_size[0]))\n",
        "as_alpha = create_as_alpha(image_size,as_alpha_border,as_alpha_feather)\n",
        "x_schedule, y_schedule = create_as_offsets(image_size,as_image_size,as_stitch_overlap)\n",
        "\n",
        "as_n_frame = 0\n",
        "as_max_frame = as_n_pass*len(x_schedule)*len(y_schedule)\n",
        "\n",
        "for nn in range(as_n_pass):\n",
        "  for xx in x_schedule:\n",
        "    for yy in y_schedule:\n",
        "\n",
        "        if nn > (len(as_stitch_shift)-1):\n",
        "          shift = np.random.choice(as_stitch_shift,1)[0]\n",
        "        else:\n",
        "          shift = as_stitch_shift[nn]\n",
        "\n",
        "        #print(xx,yy)\n",
        "        as_cuts = as_cutout_image(as_img, xx+shift, yy+shift, image_size)\n",
        "\n",
        "        w1 = xx+shift\n",
        "        w2 = xx+shift+image_size[0]\n",
        "        h1 = yy+shift\n",
        "        h2 = yy+shift+image_size[1]\n",
        "\n",
        "        w1_diff = 0\n",
        "        w2_diff = image_size[0]\n",
        "        h1_diff = 0\n",
        "        h2_diff = image_size[1]\n",
        "\n",
        "        if w1 < 0:\n",
        "          w1_diff = 0 - w1\n",
        "        if w2 > as_image_size[0]:\n",
        "          w2_diff = image_size[0]-(w2-as_image_size[0])\n",
        "\n",
        "        if h1 < 0:\n",
        "          h1_diff = 0 - h1\n",
        "        if h2 > as_image_size[1]:\n",
        "          h2_diff = image_size[1]-(h2-as_image_size[1])\n",
        "\n",
        "        as_pred = jnp.ones((1,3,image_size[1],image_size[0]))\n",
        "        as_pred = (as_pred*as_alpha)+(as_cuts*(1-as_alpha))\n",
        "        as_img = as_img.at[:,:,h1+h1_diff:h2+(image_size[1]-h2_diff),w1+w1_diff:w2+(image_size[0]-w2_diff)].set(as_pred[:,:,h1_diff:h2_diff,w1_diff:w2_diff])\n",
        "        \n",
        "        #print(\"cut\")\n",
        "        #display_images(as_cuts)\n",
        "        #print(\"pred\")\n",
        "        #display_images(as_pred)\n",
        "        #display_images(ar_img)\n",
        "\n",
        "        images = as_img.add(1).div(2).clamp(0, 1)\n",
        "        images = torch.tensor(np.array(images))\n",
        "        pil_image = TF.to_pil_image(images[0])\n",
        "        pil_image.save(f'{tmp_path}test_{str(as_n_frame).zfill(len(str(as_max_frame)))}.png')\n",
        "        as_n_frame += 1\n",
        "\n",
        "import glob\n",
        "import subprocess\n",
        "from base64 import b64encode\n",
        "\n",
        "init_frame = 0\n",
        "last_frame = as_max_frame\n",
        "fps = 6\n",
        "image_path = f\"{tmp_path}test_%0{len(str(as_max_frame))}d.png\"\n",
        "filepath = f\"{tmp_path}test.mp4\"\n",
        "\n",
        "cmd = [\n",
        "      'ffmpeg',\n",
        "      '-y',\n",
        "      '-vcodec',\n",
        "      'png',\n",
        "      '-r',\n",
        "      str(fps),\n",
        "      '-start_number',\n",
        "      str(init_frame),\n",
        "      '-i',\n",
        "      image_path,\n",
        "      '-frames:v',\n",
        "      str(last_frame),\n",
        "      '-c:v',\n",
        "      'libx264',\n",
        "      '-vf',\n",
        "      f'fps={fps}',\n",
        "      '-pix_fmt',\n",
        "      'yuv420p',\n",
        "      '-crf',\n",
        "      '17',\n",
        "      '-preset',\n",
        "      'veryslow',\n",
        "      filepath\n",
        "      ]\n",
        "\n",
        "process = subprocess.Popen(cmd, cwd=f'{tmp_path}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "  print(stderr)\n",
        "  raise RuntimeError(stderr)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "mp4 = open(filepath,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.HTML(\"\"\"\n",
        "      <video width=400 controls>\n",
        "            <source src=\"%s\" type=\"video/mp4\">\n",
        "      </video>\n",
        "      \"\"\" % data_url)\n"
      ],
      "metadata": {
        "id": "ZQrrKrKDdoHE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Settings (Optional)"
      ],
      "metadata": {
        "id": "UxWpKOkGRoZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_params():\n",
        "\n",
        "  #@markdown Batch Run Settings (Optional)\n",
        "  use_batch_runs = False #@param {type:\"boolean\"}\n",
        "  save_batch_settings = False #@param {type:\"boolean\"}\n",
        "  n_batch_runs = 100 #@param {type:\"integer\"}\n",
        "  #@markdown Batch Prompts (Optional)\n",
        "  duplicate_all_prompts = False #@param {type:\"boolean\"}\n",
        "\n",
        "  if duplicate_all_prompts:\n",
        "      batch_all_prompts = [all_prompts]*n_batch_runs\n",
        "\n",
        "  #@markdown Batch Random Settings (Optional)\n",
        "  use_random_settings = False #@param {type:\"boolean\"}\n",
        "  def random_settings():\n",
        "  \n",
        "    def rand_func(minval,maxval):\n",
        "      randval = np.random.randint(minval,maxval)\n",
        "      return(randval)\n",
        "\n",
        "    steps = rand_func(100,200)\n",
        "    eta = rand_func(-5,10)/10.0\n",
        "    clip_guidance_scale = rand_func(1000,100000)\n",
        "    tv_scale = rand_func(0,10000)\n",
        "    range_scale = rand_func(0,1000)\n",
        "    mean_scale = rand_func(-1000,1000)\n",
        "    var_scale = rand_func(-1000,1000)\n",
        "    horizontal_symmetry_scale = rand_func(-10000,10000)\n",
        "    vertical_symmetry_scale = rand_func(-10000,10000)\n",
        "\n",
        "    globals().update(locals())\n",
        "    return\n",
        "\n",
        "  #@markdown Batch Random Prompts (Optional)\n",
        "  use_random_prompts = False #@param {type:\"boolean\"}\n",
        "  subjects_path = \"subjects.csv\" #@param {type:\"string\"}\n",
        "  modifiers_path = \"modifiers.csv\" #@param {type:\"string\"}\n",
        "  artists_path = \"artists.csv\" #@param {type:\"string\"}\n",
        "  prompt_append_path = link_path #@param\n",
        "\n",
        "  def get_random_prompts(n_batch_runs):\n",
        "    import pandas as pd\n",
        "    modifiers = pd.read_csv(prompt_append_path+modifiers_path)\n",
        "    artists = pd.read_csv(prompt_append_path+artists_path)\n",
        "    subjects = pd.read_csv(prompt_append_path+subjects_path)\n",
        "    batch_all_prompts = []\n",
        "    for ii in range(n_batch_runs):\n",
        "      temp_subject = subjects.subject.sample(1).item()\n",
        "      temp_modifier = modifiers.modifier.sample(1).item()\n",
        "      temp_artist = artists.artist.sample(1).item()\n",
        "      temp_prompt = f\"concept art of a {temp_modifier} {temp_subject} by {temp_artist}:1 | text:-1.5 | watermark:-1\" #@param\n",
        "      batch_all_prompts.append(temp_prompt)\n",
        "    return batch_all_prompts\n",
        "\n",
        "  if use_random_prompts:\n",
        "    batch_all_prompts = get_random_prompts(n_batch_runs)\n",
        "\n",
        "  #@markdown Batch Initial Image Settings (Optional)\n",
        "  use_random_init = False #@param {type:\"boolean\"}\n",
        "  use_ordered_init = False #@param {type:\"boolean\"}\n",
        "  batch_init_path = \"\" #@param {type:\"string\"}\n",
        "  batch_init_append_path = link_path #@param\n",
        "\n",
        "  def get_batch_prompts(n_batch_runs):\n",
        "    import pandas as pd\n",
        "\n",
        "    if (use_random_init) or (use_ordered_init):\n",
        "      try:\n",
        "        ext = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"]\n",
        "        filenames = []\n",
        "        for filename in os.listdir(batch_init_append_path+batch_init_path):\n",
        "          if os.path.splitext(filename)[1].lower() in ext:\n",
        "            filenames.append(filename)\n",
        "        file_df = pd.DataFrame({\"init_filenames\":sorted(filenames)})\n",
        "        print(file_df)\n",
        "      except:\n",
        "        print(\"...path error\")\n",
        "\n",
        "    if use_random_init:\n",
        "      batch_all_prompts = []\n",
        "      for i in range(n_batch_runs):\n",
        "        batch_all_prompts.append(batch_init_append_path+batch_init_path+file_df.sample(1).values[0][0])\n",
        "\n",
        "    if use_ordered_init:\n",
        "      batch_all_prompts = []\n",
        "      n_batch_runs = len(file_df)\n",
        "      for i in range(n_batch_runs):\n",
        "        batch_all_prompts.append(batch_init_append_path+batch_init_path+file_df.loc[i].values[0])\n",
        "\n",
        "    return batch_all_prompts, n_batch_runs\n",
        "\n",
        "  return locals(), random_settings\n",
        "\n",
        "batch_params, random_settings = batch_params()\n",
        "globals().update(batch_params)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nq7aCWcmOkG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_all_prompts = [\"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\",\n",
        "                     \"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\",\n",
        "                     \"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\",\n",
        "                     \"Chasm City gates at night | Japanese woodblock print | trending on artstation | by Ya\"\n",
        "                    ]\n",
        "\n",
        "n_batch_runs = len(batch_all_prompts)"
      ],
      "metadata": {
        "id": "Q2V_6VwChevf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "zIQISXP1R2bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Display Rate\n",
        "use_display_rate = False #@param {type:\"boolean\"}\n",
        "display_rate = 20 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Display Percent\n",
        "use_display_percent = True #@param {type:\"boolean\"}\n",
        "display_percent = [0.4,0.6,0.8] #@param\n",
        "\n",
        "#@markdown Automated Stitching\n",
        "resume_automated_stitching = True #@param {type:\"boolean\"}\n",
        "\n",
        "def config():\n",
        "\n",
        "  # clip\n",
        "  vitb16 = lambda: get_clip('ViT-B/16')\n",
        "  vitb32 = lambda: get_clip('ViT-B/32')\n",
        "  vitl14 = lambda: get_clip('ViT-L/14')\n",
        "\n",
        "  # diffusion\n",
        "  if diffusion_model == 'OpenAIFinetune':\n",
        "    diffusion = openai_512_finetune()\n",
        "  if diffusion_model == 'CC12M':\n",
        "    diffusion = cc12m_1_wrap(cc12m_1_params(), clip_embed=process_prompts(vitb16(), title))\n",
        "\n",
        "  # secondary\n",
        "  if use_secondary_model:\n",
        "    cond_model = secondary2_wrap()\n",
        "  else:\n",
        "    cond_model = diffusion\n",
        "\n",
        "  # cuts\n",
        "  make_cutouts = MakeCutouts(224, cutn, cut_pow=cut_pow, p_grey=cut_p_grey, p_flip=cut_p_flip, p_mixgrey=cut_p_mixgrey)\n",
        "\n",
        "  # cond\n",
        "  cond_fn = MainCondFn(cond_model,[\n",
        "    CondCLIP(vitb16(), make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vitb16(), vitb16_prompt), vitb16_cgs) if use_vitb16 and vitb16_cgs > 0 else None),                               \n",
        "    CondCLIP(vitb32(), make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vitb32(), vitb32_prompt), vitb32_cgs) if use_vitb32 and vitb32_cgs > 0 else None),\n",
        "    CondCLIP(vitl14(), make_cutouts, cut_batches, SphericalDistLoss(process_prompts(vitl14(), vitl14_prompt), vitl14_cgs) if use_vitl14 and vitl14_cgs > 0 else None),\n",
        "    CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "    CondRange(range_scale) if range_scale > 0 else None,\n",
        "    CondMean(mean_scale) if mean_scale > 0 else None,\n",
        "    CondVar(var_scale) if var_scale > 0 else None,\n",
        "    CondHorizontalSymmetry(horizontal_symmetry_scale) if horizontal_symmetry_scale > 0 else None,\n",
        "    CondVerticalSymmetry(vertical_symmetry_scale) if vertical_symmetry_scale > 0 else None,\n",
        "    CondMSE(init_array, init_mse_scale) if init_mse_scale > 0 and use_init_img else None\n",
        "  ])\n",
        "    \n",
        "  return diffusion, cond_fn\n",
        "\n",
        "@torch.no_grad()\n",
        "def run():\n",
        "\n",
        "  global seed\n",
        "\n",
        "  # seed\n",
        "  if seed > 0:\n",
        "    local_seed = seed\n",
        "  else:\n",
        "    seed = int(time.time())\n",
        "  rng = PRNG(jax.random.PRNGKey(seed))\n",
        "\n",
        "  for i in range(n_batches):\n",
        "    \n",
        "    alphas, sigmas = cosine.to_alpha_sigma(schedule)\n",
        "\n",
        "    x = jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "    if init_array is not None:\n",
        "      x = sigmas[0] * x + alphas[0] * init_array\n",
        "\n",
        "    # main loop\n",
        "    if sample_mode == 'ddim':\n",
        "      sample_loop = partial(sampler.ddim_sample_loop, eta=ddim_eta)\n",
        "    if sample_mode == 'prk':\n",
        "      sample_loop = sampler.prk_sample_loop\n",
        "    if sample_mode == 'plms':\n",
        "      sample_loop = sampler.plms_sample_loop\n",
        "    \n",
        "    for output in sample_loop(diffusion, cond_fn, x, schedule, rng.split()):\n",
        "      j = output['step']\n",
        "      pred = output['pred']\n",
        "      assert x.isfinite().all().item()\n",
        "\n",
        "      # rate\n",
        "      if ((j % display_rate) == 0 and use_display_rate) and (j not in [0,len(schedule)-1]):\n",
        "        display_images(pred)\n",
        "\n",
        "      # percent\n",
        "      if ((j in display_steps) and use_display_percent) and j != len(schedule)-1:\n",
        "        display_images(pred)\n",
        "            \n",
        "    # save samples\n",
        "    #display_images(output['pred'])\n",
        "\n",
        "    return(output['pred'])\n",
        "\n",
        "# main\n",
        "try:\n",
        "\n",
        "  # display steps\n",
        "  if use_display_percent:\n",
        "    display_steps = [int(steps*percent) for percent in display_percent]\n",
        "  else:\n",
        "    display_steps = []\n",
        "\n",
        "  # init\n",
        "  if use_automated_stitching and resume_automated_stitching:\n",
        "    xx_start, yy_start, as_n_start = ix, iy, nn\n",
        "  elif use_automated_stitching:\n",
        "    as_img = load_image(as_init_append_path+as_init_path,as_image_size)\n",
        "    as_alpha = create_as_alpha(image_size,as_alpha_border,as_alpha_feather)\n",
        "    x_schedule, y_schedule = create_as_offsets(image_size,as_image_size,as_stitch_overlap)\n",
        "    xx_start, yy_start = 0, 0, 0\n",
        "  elif use_init_img:\n",
        "    init_array = load_image(init_append_path+init_path,image_size)\n",
        "    as_n_pass, x_schedule, y_schedule = 1, [1], [1]\n",
        "  else:\n",
        "    init_array = None\n",
        "    as_n_pass, x_schedule, y_schedule = 1, [1], [1]\n",
        "\n",
        "  for nn in range(as_n_start,as_n_pass):\n",
        "    if use_automated_stitching:\n",
        "      print(f\"...starting pass {nn+1}\")\n",
        "    else:\n",
        "      print(f\"...starting run\")\n",
        "    for ix in range(xx_start,len(x_schedule)):\n",
        "      for iy in range(yy_start,len(y_schedule)):\n",
        "\n",
        "          # preparation\n",
        "          timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "          \n",
        "          # clip\n",
        "          title = expand([all_prompts], batch_size)\n",
        "          vitb16_cgs = all_clip_guidance_scale\n",
        "          vitb16_prompt = title\n",
        "          vitb32_cgs = all_clip_guidance_scale\n",
        "          vitb32_prompt = title\n",
        "          vitl14_cgs = all_clip_guidance_scale\n",
        "          vitl14_prompt = title\n",
        "\n",
        "          if use_multiple_prompts:\n",
        "            vitb16_cgs = vitb16_clip_guidance_scale\n",
        "            vitb16_prompt = expand([vitb16_all_prompt], batch_size)\n",
        "            vitb32_cgs = vitb32_clip_guidance_scale\n",
        "            vitb32_prompt = expand([vitb32_all_prompt], batch_size)\n",
        "            vitl14_cgs = vitl14_clip_guidance_scale\n",
        "            vitl14_prompt = expand([vitl14_all_prompt], batch_size)\n",
        "          \n",
        "          schedule = jnp.linspace(starting_noise, ending_noise, steps+1)\n",
        "          schedule = spliced.to_cosine(schedule)\n",
        "          out_path = get_output_folder(output_path, batch_folder)\n",
        "          diffusion, cond_fn = config()\n",
        "\n",
        "          if nn == 0:\n",
        "            with open(f\"{out_path}{timestring}.txt\", \"w+\") as f:\n",
        "              json.dump(params, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "          if use_automated_stitching:\n",
        "\n",
        "            xx = x_schedule[ix]\n",
        "            yy = y_schedule[iy]\n",
        "\n",
        "            if nn > (len(as_stitch_shift)-1):\n",
        "              shift = np.random.choice(as_stitch_shift,1)[0]\n",
        "            else:\n",
        "              shift = as_stitch_shift[nn]\n",
        "\n",
        "            #print(xx,yy)\n",
        "            as_cuts = as_cutout_image(as_img, xx+shift, yy+shift, image_size)\n",
        "            init_array = as_cuts\n",
        "\n",
        "            w1 = xx+shift\n",
        "            w2 = xx+shift+image_size[0]\n",
        "            h1 = yy+shift\n",
        "            h2 = yy+shift+image_size[1]\n",
        "\n",
        "            w1_diff = 0\n",
        "            w2_diff = image_size[0]\n",
        "            h1_diff = 0\n",
        "            h2_diff = image_size[1]\n",
        "\n",
        "            if w1 < 0:\n",
        "              w1_diff = 0 - w1\n",
        "            if w2 > as_image_size[0]:\n",
        "              w2_diff = image_size[0]-(w2-as_image_size[0])\n",
        "\n",
        "            if h1 < 0:\n",
        "              h1_diff = 0 - h1\n",
        "            if h2 > as_image_size[1]:\n",
        "              h2_diff = image_size[1]-(h2-as_image_size[1])\n",
        "\n",
        "            as_pred = run()\n",
        "            as_pred = (as_pred*as_alpha)+(as_cuts*(1-as_alpha))\n",
        "            as_img = as_img.at[:,:,h1+h1_diff:h2+(image_size[1]-h2_diff),w1+w1_diff:w2+(image_size[0]-w2_diff)].set(as_pred[:,:,h1_diff:h2_diff,w1_diff:w2_diff])\n",
        "          \n",
        "            #display_images(as_cuts)\n",
        "            #display_images(as_pred)\n",
        "            #display_images(as_img)\n",
        "\n",
        "          else:\n",
        "            pred = run()\n",
        "            \n",
        "            # save samples\n",
        "            display_images(pred)\n",
        "            images = pred.add(1).div(2).clamp(0, 1)\n",
        "            images = torch.tensor(np.array(images))\n",
        "            for k in range(batch_size):\n",
        "              pil_image = TF.to_pil_image(images[k])\n",
        "              pil_image.save(f'{out_path}{timestring}_{k}.png')\n",
        "\n",
        "    if use_automated_stitching:\n",
        "      display_images(as_img)\n",
        "      images = as_img.add(1).div(2).clamp(0, 1)\n",
        "      images = torch.tensor(np.array(images))\n",
        "      pil_image = TF.to_pil_image(images[0])\n",
        "      pil_image.save(f'{out_path}{timestring}.png')\n",
        "\n",
        "    success = True\n",
        "\n",
        "except:\n",
        "  import traceback\n",
        "  traceback.print_exc()\n",
        "  success = False\n",
        "assert success"
      ],
      "metadata": {
        "id": "vHrU5fZi7Qun",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(as_img)"
      ],
      "metadata": {
        "id": "wJI_U1c8igmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}